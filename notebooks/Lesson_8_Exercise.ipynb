{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "lecture 8 code exercises.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QruKDaClNLUp"
      },
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9TOGRZgNLUs"
      },
      "source": [
        "from fastbook import *\n",
        "from fastai.text.all import *\n",
        "from IPython.display import display,HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4mSsUOhXzsZ"
      },
      "source": [
        "We'll be looking at the IMDB dataset, so download it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7RIQGu5cXxeg",
        "outputId": "079e38e1-b642-4825-c39c-feab59a81a34"
      },
      "source": [
        "path = untar_data(URLs.IMDB)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf0qR1CcNLUt"
      },
      "source": [
        "# Conceptual review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkQbORzDNLUt"
      },
      "source": [
        "In this notebook, we are going to review some of the content from Chapter 10 and explore how alternative choices would effect the final outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nubaN4ZrNLUv"
      },
      "source": [
        "## Question 1: What changes would need to be made to the notebook from Chapter 10 to use Subword Tokenization? Why might you want to use it?\n",
        "\n",
        "Consider the pretrained model and the fine tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY39vT7-QEh6"
      },
      "source": [
        "Answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrZ9IWqHNLVL"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6r_s7ItVPQw"
      },
      "source": [
        "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
        "dls_lm = DataBlock(\n",
        "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
        "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
        ").dataloaders(path, path=path, bs=128, seq_len=80)\n",
        "\n",
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
        "    metrics=[accuracy, Perplexity()]).to_fp16()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwhGTrwdNLVL"
      },
      "source": [
        "Before we move on to fine-tuning the classifier, let's quickly try something different: using our model to generate random reviews. Since it's trained to guess what the next word of the sentence is, we can use the model to write new reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0hHSc3lNLVL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2e5527d8-7483-460f-8a54-472342199838"
      },
      "source": [
        "TEXT = \"I liked this movie because\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
        "         for _ in range(N_SENTENCES)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NQ3pNrjNLVL"
      },
      "source": [
        "print(\"\\n\".join(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncV2tmw9ts98"
      },
      "source": [
        "With a fine tuned language model Jeremy got the following results\n",
        "```\n",
        "i liked this movie because of its story and characters . The story line was very strong , very good for a sci - fi film . The main character , Alucard , was very well developed and brought the whole story\n",
        "i liked this movie because i like the idea of the premise of the movie , the ( very ) convenient virus ( which , when you have to kill a few people , the \" evil \" machine has to be used to protect\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N1PcxPsNLVM"
      },
      "source": [
        "## Question 2\n",
        "In the section above, Jermy has shown us how convincingly the fine tuned language model can generate movie reviews. Your task is to explore generating examples from a language model that hasn't been fine tuned. What can we say about what the fine tuning has taught the language model?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTtEj3iESdOM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQXo-iD0NLVM"
      },
      "source": [
        "## Question 3\n",
        "The fine tuned classifcation model in Chapter 10 reached 94.3% accuracy, explore how close you can get without starting with a fine tuned language model.\n",
        "\n",
        "While this model trains, try out question 4.\n",
        "\n",
        "Think about whether the language model fine tuning is worth the time it takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_lw1ZHkNLVM"
      },
      "source": [
        "dls_clas = DataBlock(\n",
        "    blocks=(TextBlock.from_folder(path),CategoryBlock),\n",
        "    get_y = parent_label,\n",
        "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
        "    splitter=GrandparentSplitter(valid_name='test')\n",
        ").dataloaders(path, path=path, bs=128, seq_len=72)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCgpO5YpNLVN"
      },
      "source": [
        "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
        "                                metrics=accuracy).to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF_SRKD_NLVO"
      },
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxs8QjYiNLVO"
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brrgIgcANLVP"
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETU9R5aTNLVP"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8ysFOHLZYQI"
      },
      "source": [
        "## Question 4\n",
        "In the `Creating the Classifier DataLoaders` section of https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb, the classifcation data loader gets defined with a seq_len of 72. Later in the section Jeremy explains \"`We will expand the shortest texts to make them all the same size.`\"  Some of the samples in the first batch have over 500 tokens. Explain how seq_len can be 72 with so many tokens in the samples.\n",
        "\n",
        "Hint start looking at (https://github.com/fastai/fastai/blob/99d38fec7207db9b4209568bebc85ded7e3d3f1b/fastai/text/models/core.py#L115)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPtKK-OuiHeU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqW284ZNNLVR"
      },
      "source": [
        "## Questionnaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VPz2kfENLVR"
      },
      "source": [
        "1. What is \"self-supervised learning\"?\n",
        "1. What is a \"language model\"?\n",
        "1. Why is a language model considered self-supervised?\n",
        "1. What are self-supervised models usually used for?\n",
        "1. Why do we fine-tune language models?\n",
        "1. What are the three steps to create a state-of-the-art text classifier?\n",
        "1. How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n",
        "1. What are the three steps to prepare your data for a language model?\n",
        "1. What is \"tokenization\"? Why do we need it?\n",
        "1. Name three different approaches to tokenization.\n",
        "1. What is `xxbos`?\n",
        "1. List four rules that fastai applies to text during tokenization.\n",
        "1. Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\n",
        "1. What is \"numericalization\"?\n",
        "1. Why might there be words that are replaced with the \"unknown word\" token?\n",
        "1. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book's website.)\n",
        "1. Why do we need padding for text classification? Why don't we need it for language modeling?\n",
        "1. What does an embedding matrix for NLP contain? What is its shape?\n",
        "1. What is \"perplexity\"?\n",
        "1. Why do we have to pass the vocabulary of the language model to the classifier data block?\n",
        "1. What is \"gradual unfreezing\"?\n",
        "1. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?"
      ]
    }
  ]
}